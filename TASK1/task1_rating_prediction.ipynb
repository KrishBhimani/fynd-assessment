{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Rating Prediction via Prompting\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **star rating prediction system** for Yelp reviews using Large Language Models (LLMs) via LangChain and OpenAI.\n",
    "\n",
    "### What This Notebook Evaluates\n",
    "- **Zero-shot prompting capability**: Can an LLM accurately predict star ratings (1-5) from review text without fine-tuning?\n",
    "- **Structured output reliability**: How well does the LLM adhere to a strict JSON schema?\n",
    "- **Baseline performance**: Establishes Prompt V1 metrics for comparison with optimized prompts.\n",
    "\n",
    "### High-Level Workflow\n",
    "1. Load and sample the Yelp reviews dataset (stratified by star rating)\n",
    "2. Define a structured output schema using Pydantic\n",
    "3. Construct an LCEL (LangChain Expression Language) pipeline\n",
    "4. Run batch predictions on the sampled dataset\n",
    "5. Compute evaluation metrics (accuracy, JSON validity rate)\n",
    "6. Persist results for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports & Environment Setup\n",
    "\n",
    "We use **modern LangChain APIs** (LCEL/Runnable pipelines) and **Pydantic v2** for structured output validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain - Modern imports (LCEL compatible)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Pydantic v2 for structured output\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY validated: sk-proj-...tyIA\n"
     ]
    }
   ],
   "source": [
    "# Validate OpenAI API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY environment variable is not set. \"\n",
    "        \"Please set it before running this notebook:\\n\"\n",
    "        \"  Windows: set OPENAI_API_KEY=your-key-here\\n\"\n",
    "        \"  Linux/Mac: export OPENAI_API_KEY=your-key-here\"\n",
    "    )\n",
    "\n",
    "# Mask the key for display\n",
    "masked_key = OPENAI_API_KEY[:8] + \"...\" + OPENAI_API_KEY[-4:]\n",
    "print(f\"âœ… OPENAI_API_KEY validated: {masked_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset Loading & Sampling\n",
    "\n",
    "We load the Yelp reviews dataset and perform **stratified sampling** to ensure balanced representation across all star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset loaded: 10,000 reviews\n",
      "ðŸ“‹ Columns: ['business_id', 'date', 'review_id', 'stars', 'text', 'type', 'user_id', 'cool', 'useful', 'funny']\n",
      "\n",
      "â­ Star distribution:\n",
      "stars\n",
      "1     749\n",
      "2     927\n",
      "3    1461\n",
      "4    3526\n",
      "5    3337\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "DATASET_PATH = \"yelp.csv\"\n",
    "\n",
    "df_full = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print(f\"ðŸ“Š Dataset loaded: {len(df_full):,} reviews\")\n",
    "print(f\"ðŸ“‹ Columns: {df_full.columns.tolist()}\")\n",
    "print(f\"\\nâ­ Star distribution:\")\n",
    "print(df_full['stars'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Sampled dataset: 200 reviews\n",
      "\n",
      "â­ Sampled star distribution:\n",
      "stars\n",
      "1    40\n",
      "2    40\n",
      "3    40\n",
      "4    40\n",
      "5    40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stratified sampling: ~40 reviews per star rating = 200 total\n",
    "SAMPLES_PER_STAR = 40\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def stratified_sample(df: pd.DataFrame, n_per_class: int, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform stratified sampling by star rating.\n",
    "    \n",
    "    Args:\n",
    "        df: Source dataframe\n",
    "        n_per_class: Number of samples per star rating\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Stratified sample dataframe\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for star in range(1, 6):\n",
    "        star_df = df[df['stars'] == star]\n",
    "        n_available = len(star_df)\n",
    "        n_sample = min(n_per_class, n_available)\n",
    "        samples.append(star_df.sample(n=n_sample, random_state=seed))\n",
    "    \n",
    "    return pd.concat(samples, ignore_index=True)\n",
    "\n",
    "# Create stratified sample\n",
    "df_sample = stratified_sample(df_full, SAMPLES_PER_STAR, RANDOM_SEED)\n",
    "\n",
    "# Keep only required columns\n",
    "df_sample = df_sample[['review_id', 'text', 'stars']].copy()\n",
    "\n",
    "print(f\"ðŸ“Š Sampled dataset: {len(df_sample)} reviews\")\n",
    "print(f\"\\nâ­ Sampled star distribution:\")\n",
    "print(df_sample['stars'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No null text values\n",
      "âœ… All star values are in range [1, 5]\n",
      "âœ… All review IDs are unique\n",
      "âœ… All reviews have sufficient text length\n",
      "\n",
      "ðŸŽ¯ Dataset validation complete: 200 reviews ready for inference\n"
     ]
    }
   ],
   "source": [
    "# Data validation assertions\n",
    "\n",
    "# Assert no null text values\n",
    "assert df_sample['text'].notna().all(), \"Found null text values in sample!\"\n",
    "print(\"âœ… No null text values\")\n",
    "\n",
    "# Assert star values are in valid range [1, 5]\n",
    "assert df_sample['stars'].isin([1, 2, 3, 4, 5]).all(), \"Found invalid star values!\"\n",
    "print(\"âœ… All star values are in range [1, 5]\")\n",
    "\n",
    "# Assert no duplicate review IDs\n",
    "assert df_sample['review_id'].nunique() == len(df_sample), \"Found duplicate review IDs!\"\n",
    "print(\"âœ… All review IDs are unique\")\n",
    "\n",
    "# Assert minimum text length\n",
    "assert (df_sample['text'].str.len() > 10).all(), \"Found reviews with very short text!\"\n",
    "print(\"âœ… All reviews have sufficient text length\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Dataset validation complete: {len(df_sample)} reviews ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Output Schema Definition\n",
    "\n",
    "We define a **Pydantic model** for structured LLM output. This ensures:\n",
    "- Type safety (predicted_stars must be an integer)\n",
    "- Value constraints (stars between 1-5)\n",
    "- Required fields (explanation must be non-empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Schema validated: {'predicted_stars': 4, 'explanation': 'Great review with positive sentiment'}\n"
     ]
    }
   ],
   "source": [
    "class RatingPrediction(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured output schema for rating prediction.\n",
    "    \n",
    "    Attributes:\n",
    "        predicted_stars: The predicted star rating (1-5)\n",
    "        explanation: Reasoning for the prediction\n",
    "    \"\"\"\n",
    "    predicted_stars: int = Field(\n",
    "        ...,\n",
    "        description=\"Predicted star rating from 1 to 5\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=\"Brief explanation for the predicted rating\",\n",
    "        min_length=1\n",
    "    )\n",
    "    \n",
    "    @field_validator('predicted_stars')\n",
    "    @classmethod\n",
    "    def validate_stars_range(cls, v: int) -> int:\n",
    "        \"\"\"Ensure predicted_stars is within valid range.\"\"\"\n",
    "        if v < 1 or v > 5:\n",
    "            raise ValueError(f\"predicted_stars must be between 1 and 5, got {v}\")\n",
    "        return v\n",
    "    \n",
    "    @field_validator('explanation')\n",
    "    @classmethod\n",
    "    def validate_explanation_not_empty(cls, v: str) -> str:\n",
    "        \"\"\"Ensure explanation is not empty or whitespace only.\"\"\"\n",
    "        if not v or not v.strip():\n",
    "            raise ValueError(\"explanation must be non-empty\")\n",
    "        return v.strip()\n",
    "\n",
    "# Test the schema\n",
    "test_prediction = RatingPrediction(predicted_stars=4, explanation=\"Great review with positive sentiment\")\n",
    "print(f\"âœ… Schema validated: {test_prediction.model_dump()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Output Parser\n",
    "\n",
    "We use `PydanticOutputParser` to:\n",
    "1. **Generate format instructions** for the LLM (tells it exactly how to structure output)\n",
    "2. **Parse and validate** LLM responses against our schema\n",
    "\n",
    "### Why Strict Parsing Matters\n",
    "- **Reliability**: Ensures consistent, machine-readable output\n",
    "- **Error Detection**: Catches malformed responses early\n",
    "- **Type Safety**: Guarantees correct data types for downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Format Instructions for LLM:\n",
      "--------------------------------------------------\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Structured output schema for rating prediction.\\n\\nAttributes:\\n    predicted_stars: The predicted star rating (1-5)\\n    explanation: Reasoning for the prediction\", \"properties\": {\"predicted_stars\": {\"description\": \"Predicted star rating from 1 to 5\", \"maximum\": 5, \"minimum\": 1, \"title\": \"Predicted Stars\", \"type\": \"integer\"}, \"explanation\": {\"description\": \"Brief explanation for the predicted rating\", \"minLength\": 1, \"title\": \"Explanation\", \"type\": \"string\"}}, \"required\": [\"predicted_stars\", \"explanation\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Initialize the output parser\n",
    "output_parser = PydanticOutputParser(pydantic_object=RatingPrediction)\n",
    "\n",
    "# Get format instructions for the prompt\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(\"ðŸ“‹ Format Instructions for LLM:\")\n",
    "print(\"-\" * 50)\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. LLM Initialization\n",
    "\n",
    "We use `ChatOpenAI` with:\n",
    "- **temperature=0**: Deterministic outputs for reproducibility\n",
    "- **gpt-4o-mini**: Cost-effective model with strong capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM initialized: gpt-4.1-nano\n",
      "   Temperature: 0.2\n",
      "   Max retries: 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    temperature=0.2,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    max_retries=2,\n",
    "    request_timeout=60\n",
    ")\n",
    "\n",
    "print(f\"âœ… LLM initialized: {llm.model_name}\")\n",
    "print(f\"   Temperature: {llm.temperature}\")\n",
    "print(f\"   Max retries: {llm.max_retries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Prompt Construction (Baseline - Prompt V1)\n",
    "\n",
    "This is our **baseline prompt** without any optimization. It follows best practices:\n",
    "- **System message**: Defines the role and task\n",
    "- **Human message**: Contains the review text and format instructions\n",
    "\n",
    "Future iterations can optimize this prompt for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt_template1 = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an expert sentiment analyst specializing in customer reviews.\n",
    "\n",
    "Your task is to predict the star rating (1 to 5) of a Yelp review based on its text content.\n",
    "\n",
    "Rating Guidelines:\n",
    "- 1 star: Very negative experience, strong complaints\n",
    "- 2 stars: Negative experience with some issues\n",
    "- 3 stars: Mixed or neutral experience\n",
    "- 4 stars: Positive experience with minor issues\n",
    "- 5 stars: Excellent experience, highly positive\n",
    "\n",
    "Analyze the sentiment, tone, and specific feedback in the review to make your prediction.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"Please analyze the following Yelp review and predict its star rating.\n",
    "\n",
    "REVIEW:\n",
    "{review_text}\n",
    "\n",
    "---\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Provide your prediction in the exact JSON format specified above.\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "prompt_template2 = ChatPromptTemplate.from_messages([(\"system\",\"\"\"\n",
    "<SYSTEM>\n",
    "You are an expert AI specialized in predicting Yelp star ratings (1-5) from review text and providing a brief, grounded explanation.\n",
    "</SYSTEM>\n",
    "\n",
    "<TASK>\n",
    "Predict the most likely integer star rating (1-5) a Yelp user would give based solely on the review text.\n",
    "Provide a concise explanation that directly reflects the evidence in the review.\n",
    "</TASK>\n",
    "\n",
    "<DECISION_PROCESS>\n",
    "Follow this decision process internally before producing the final answer.\n",
    "\n",
    "<STEP_1_OVERALL_DIRECTION>\n",
    "First, determine the overall direction of the experience described in the review:\n",
    "- Positive\n",
    "- Neutral\n",
    "- Negative\n",
    "</STEP_1_OVERALL_DIRECTION>\n",
    "\n",
    "<STEP_2_SIGNAL_EXTRACTION>\n",
    "Identify concrete signals from the review:\n",
    "\n",
    "<POSITIVE_SIGNALS>\n",
    "Food quality, friendly or attentive service, good value, satisfaction, recommendations, intent to return.\n",
    "</POSITIVE_SIGNALS>\n",
    "\n",
    "<NEGATIVE_SIGNALS>\n",
    "Long waits, being ignored, rude or inattentive staff, poor quality, price complaints, disappointment, wasted time.\n",
    "</NEGATIVE_SIGNALS>\n",
    "</STEP_2_SIGNAL_EXTRACTION>\n",
    "\n",
    "<STEP_3_YELP_SPECIFIC_RULES>\n",
    "Apply these Yelp-specific rules strictly:\n",
    "- Service failures (e.g., being ignored, excessive wait times, inattentive staff) strongly cap the rating at 2.\n",
    "- Statements implying regret, disappointment, wasted time, or not returning cap the rating at 3 or below.\n",
    "- Price complaints significantly reduce ratings unless clearly offset by strong value.\n",
    "- Ambiance or atmosphere alone cannot outweigh poor service or quality issues.\n",
    "</STEP_3_YELP_SPECIFIC_RULES>\n",
    "\n",
    "<STEP_4_RATING_SELECTION>\n",
    "Based on the overall direction and applied rules, assign a single precise star rating (1-5) that best represents how a Yelp user would rate this experience.\n",
    "</STEP_4_RATING_SELECTION>\n",
    "</DECISION_PROCESS>\n",
    "\n",
    "<RATING_CALIBRATION>\n",
    "5: Strongly positive, enthusiastic, clear satisfaction and intent to return  \n",
    "4: Mostly positive with minor issues  \n",
    "3: Balanced positives and negatives or average experience  \n",
    "2: Mostly negative with limited positives  \n",
    "1: Extremely negative, severe dissatisfaction  \n",
    "</RATING_CALIBRATION>\n",
    "\n",
    "<SHORT_REVIEW_HANDLING>\n",
    "If the review is very short or vague, rely on decisive keywords and implied intent rather than assumptions or inferred context.\n",
    "</SHORT_REVIEW_HANDLING>\n",
    "\n",
    "<OUTPUT_RULES>\n",
    "- Follow {format_instructions} exactly.\n",
    "- Output valid JSON only.\n",
    "- Do not include extra fields, text, or commentary.\n",
    "</OUTPUT_RULES>\n",
    "\n",
    "\n",
    "\"\"\"),\n",
    "(\"human\",\"\"\"Please analyze the following Yelp review and predict its star rating.\n",
    "\n",
    "REVIEW:\n",
    "{review_text}\n",
    "\n",
    "---\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Provide your prediction in the exact JSON format specified above.\n",
    "\"\"\")])\n",
    "\n",
    "prompt_template3 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"\n",
    "# Role & Objective\n",
    "\n",
    "You are an expert AI system that predicts **Yelp star ratings (1â€“5)** from user review text and provides a **brief, evidence-based explanation** grounded strictly in the review content.\n",
    "\n",
    "Your goal is to replicate how a real Yelp user would assign a star rating.\n",
    "\n",
    "---\n",
    "\n",
    "# Core Instructions (Follow Exactly)\n",
    "\n",
    "- Use **only the provided review text** as evidence.\n",
    "- Do **not** rely on external knowledge, assumptions, or unstated context.\n",
    "- Assign **one integer star rating only (1â€“5)**.\n",
    "- Provide a **concise explanation** that explicitly references signals from the review.\n",
    "- If information is insufficient, make a decision based on **explicit keywords and implied sentiment**, not speculation.\n",
    "\n",
    "---\n",
    "\n",
    "# Required Reasoning Workflow (Must Be Reflected in Output)\n",
    "\n",
    "You must follow and **explicitly show** the following steps in order before finalizing the rating:\n",
    "\n",
    "## Step 1: Determine Overall Sentiment Direction\n",
    "Classify the experience as one of:\n",
    "- **Positive**\n",
    "- **Neutral**\n",
    "- **Negative**\n",
    "\n",
    "## Step 2: Extract Concrete Signals from the Review\n",
    "\n",
    "### Positive Signals\n",
    "- Food quality  \n",
    "- Friendly or attentive service  \n",
    "- Good value for money  \n",
    "- Satisfaction or enjoyment  \n",
    "- Recommendations  \n",
    "- Intent to return  \n",
    "\n",
    "### Negative Signals\n",
    "- Long waits or delays  \n",
    "- Being ignored  \n",
    "- Rude or inattentive staff  \n",
    "- Poor food or service quality  \n",
    "- Price complaints  \n",
    "- Disappointment, regret, or wasted time  \n",
    "\n",
    "Only count signals that are **explicitly stated or strongly implied**.\n",
    "\n",
    "## Step 3: Apply Yelp-Specific Rating Rules (Strict)\n",
    "\n",
    "Apply these rules literally:\n",
    "\n",
    "- **Service failures** (ignored, rude staff, excessive wait) â†’ rating **cannot exceed 2**\n",
    "- **Regret or â€œwould not returnâ€ statements** â†’ rating **cannot exceed 3**\n",
    "- **Price complaints** â†’ lower the rating unless clearly offset by strong value\n",
    "- **Ambiance alone** cannot outweigh service or quality failures\n",
    "\n",
    "If multiple rules apply, the **most restrictive rule wins**.\n",
    "\n",
    "## Step 4: Select Final Rating\n",
    "Choose the **single best integer rating (1â€“5)** that aligns with:\n",
    "- The overall sentiment\n",
    "- The extracted signals\n",
    "- The Yelp-specific rules\n",
    "\n",
    "---\n",
    "\n",
    "# Rating Calibration Reference\n",
    "\n",
    "Use this scale exactly:\n",
    "\n",
    "- **5** â€” Strongly positive, enthusiastic, clear satisfaction and intent to return  \n",
    "- **4** â€” Mostly positive with minor issues  \n",
    "- **3** â€” Balanced positives and negatives, average experience  \n",
    "- **2** â€” Mostly negative with limited positives  \n",
    "- **1** â€” Extremely negative, severe dissatisfaction  \n",
    "\n",
    "---\n",
    "\n",
    "# Short or Ambiguous Reviews\n",
    "\n",
    "If the review is **very short or vague**:\n",
    "- Identify **high-impact keywords** (e.g., â€œgreat,â€ â€œterrible,â€ â€œnever againâ€)\n",
    "- Infer sentiment **only from those words**\n",
    "- Avoid filling in missing details\n",
    "\n",
    "---\n",
    "\n",
    "# Output Format (Mandatory)\n",
    "\n",
    "- Output **valid JSON only**\n",
    "- Follow `{format_instructions}` **exactly**\n",
    "- Do **not** include extra text, explanations, or fields outside the JSON\n",
    "\n",
    "\"\"\"),\n",
    "\n",
    "(\"human\",\"\"\"\n",
    "Please analyze the following Yelp review and predict its star rating.\n",
    "\n",
    "REVIEW:\n",
    "{review_text}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Provide your prediction in the exact JSON format specified above.\n",
    "\"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Chain Construction (Modern LCEL)\n",
    "\n",
    "We build the pipeline using **LangChain Expression Language (LCEL)**:\n",
    "\n",
    "```\n",
    "prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This is the modern, recommended approach that:\n",
    "- Avoids deprecated `LLMChain` classes\n",
    "- Provides better composability\n",
    "- Supports async, batching, and streaming out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LCEL chain\n",
    "chain1 = prompt_template1 | llm | output_parser\n",
    "chain2 = prompt_template2 | llm | output_parser\n",
    "chain3 = prompt_template3 | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Test prediction:\n",
      "   Review: The food was absolutely amazing! Best sushi I've e...\n",
      "   Predicted stars: 5\n",
      "   Explanation: The review uses highly positive language, praising the food as 'absolutely amazing' and 'best sushi ever,' along with mentioning 'impeccable' service, indicating an excellent experience deserving of a 5-star rating.\n"
     ]
    }
   ],
   "source": [
    "# Test the chain with a sample review\n",
    "test_review = \"The food was absolutely amazing! Best sushi I've ever had. Service was impeccable.\"\n",
    "\n",
    "test_result = chain1.invoke({\n",
    "    \"review_text\": test_review,\n",
    "    \"format_instructions\": format_instructions\n",
    "})\n",
    "\n",
    "print(\"ðŸ§ª Test prediction:\")\n",
    "print(f\"   Review: {test_review[:50]}...\")\n",
    "print(f\"   Predicted stars: {test_result.predicted_stars}\")\n",
    "print(f\"   Explanation: {test_result.explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Prediction Function\n",
    "\n",
    "A reusable function that:\n",
    "- Takes review text as input\n",
    "- Returns the parsed prediction, JSON validity flag, and any errors\n",
    "- Handles parsing errors and LLM failures gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(\n",
    "    review_text: str,\n",
    "    chain: Any,\n",
    "    format_instructions: str\n",
    ") -> Tuple[Optional[RatingPrediction], bool, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Predict the star rating for a review.\n",
    "    \n",
    "    Args:\n",
    "        review_text: The review text to analyze\n",
    "        chain: The LCEL chain for prediction\n",
    "        format_instructions: Format instructions for the LLM\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - prediction: RatingPrediction object (or None if failed)\n",
    "        - json_valid: Whether the output was valid JSON\n",
    "        - error: Error message (or None if successful)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Invoke the chain\n",
    "        prediction = chain.invoke({\n",
    "            \"review_text\": review_text,\n",
    "            \"format_instructions\": format_instructions\n",
    "        })\n",
    "        \n",
    "        # If we get here, parsing was successful\n",
    "        return prediction, True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Check if it's a parsing error vs LLM error\n",
    "        if \"validation error\" in error_msg.lower() or \"json\" in error_msg.lower():\n",
    "            return None, False, f\"Parsing error: {error_msg[:200]}\"\n",
    "        else:\n",
    "            return None, False, f\"LLM error: {error_msg[:200]}\"\n",
    "\n",
    "print(\"âœ… Prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Batch Inference Loop\n",
    "\n",
    "Run predictions on the entire sampled dataset, storing:\n",
    "- Review ID and text\n",
    "- Actual vs predicted stars\n",
    "- Explanation\n",
    "- JSON validity status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running inference for Prompt V1 on 200 reviews...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt v1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [03:36<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Prompt V1 Complete\n",
      "   Accuracy: 69.5%\n",
      "   JSON Validity: 100.0%\n",
      "   Saved: task1_prompt_v1_results.csv, task1_prompt_v1_results.json\n",
      "\n",
      "ðŸš€ Running inference for Prompt V2 on 200 reviews...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt v2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [03:38<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Prompt V2 Complete\n",
      "   Accuracy: 65.0%\n",
      "   JSON Validity: 100.0%\n",
      "   Saved: task1_prompt_v2_results.csv, task1_prompt_v2_results.json\n",
      "\n",
      "ðŸš€ Running inference for Prompt V3 on 200 reviews...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt v3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:07<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Prompt V3 Complete\n",
      "   Accuracy: 68.5%\n",
      "   JSON Validity: 100.0%\n",
      "   Saved: task1_prompt_v3_results.csv, task1_prompt_v3_results.json\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ All prompts evaluated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define chains and their names\n",
    "chains = {\n",
    "    \"v1\": chain1,  # Baseline prompt\n",
    "    \"v2\": chain2,  # Improved prompt\n",
    "    \"v3\": chain3   # Optimized prompt\n",
    "}\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "for prompt_version, chain in chains.items():\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nðŸš€ Running inference for Prompt {prompt_version.upper()} on {len(df_sample)} reviews...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=f\"Prompt {prompt_version}\"):\n",
    "        review_id = row['review_id']\n",
    "        review_text = row['text']\n",
    "        actual_stars = row['stars']\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction, json_valid, error = predict_rating(\n",
    "            review_text=review_text,\n",
    "            chain=chain,\n",
    "            format_instructions=format_instructions\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            \"review_id\": review_id,\n",
    "            \"review_text\": review_text[:500],\n",
    "            \"actual_stars\": actual_stars,\n",
    "            \"predicted_stars\": prediction.predicted_stars if prediction else None,\n",
    "            \"explanation\": prediction.explanation if prediction else None,\n",
    "            \"json_valid\": json_valid,\n",
    "            \"error\": error\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame and store\n",
    "    df_results = pd.DataFrame(results)\n",
    "    all_results[prompt_version] = df_results\n",
    "    \n",
    "    # Save CSV and JSON for this prompt version\n",
    "    csv_file = f\"task1_prompt_{prompt_version}_results.csv\"\n",
    "    json_file = f\"task1_prompt_{prompt_version}_results.json\"\n",
    "    \n",
    "    df_results.to_csv(csv_file, index=False)\n",
    "    \n",
    "    # Create JSON in Fynd format\n",
    "    json_output = [\n",
    "        {\n",
    "            \"predicted_stars\": int(row[\"predicted_stars\"]) if pd.notna(row[\"predicted_stars\"]) else None,\n",
    "            \"explanation\": row[\"explanation\"] if pd.notna(row[\"explanation\"]) else None\n",
    "        }\n",
    "        for _, row in df_results.iterrows()\n",
    "    ]\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Print summary\n",
    "    valid_df = df_results[df_results['json_valid']]\n",
    "    accuracy = (valid_df['actual_stars'] == valid_df['predicted_stars']).mean() * 100 if len(valid_df) > 0 else 0\n",
    "    json_rate = df_results['json_valid'].mean() * 100\n",
    "    \n",
    "    print(f\"\\nâœ… Prompt {prompt_version.upper()} Complete\")\n",
    "    print(f\"   Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"   JSON Validity: {json_rate:.1f}%\")\n",
    "    print(f\"   Saved: {csv_file}, {json_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ All prompts evaluated!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Evaluation Metrics\n",
    "\n",
    "Compute and display:\n",
    "- **Accuracy**: Percentage of correct predictions (valid JSON only)\n",
    "- **JSON Validity Rate**: Percentage of successfully parsed responses\n",
    "- **Per-Star Metrics**: Accuracy breakdown by actual star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š EVALUATION METRICS - ALL PROMPTS\n",
      "============================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Œ PROMPT V1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸ”— JSON Validity Rate: 100.0%\n",
      "   Valid: 200 / 200\n",
      "\n",
      "ðŸŽ¯ Overall Accuracy: 69.5%\n",
      "   Correct: 139 / 200\n",
      "\n",
      "ðŸ“ˆ Per-Star Accuracy:\n",
      "   1â­:  70.0% (28 / 40)\n",
      "   2â­:  60.0% (24 / 40)\n",
      "   3â­:  67.5% (27 / 40)\n",
      "   4â­:  75.0% (30 / 40)\n",
      "   5â­:  75.0% (30 / 40)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Œ PROMPT V2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸ”— JSON Validity Rate: 100.0%\n",
      "   Valid: 200 / 200\n",
      "\n",
      "ðŸŽ¯ Overall Accuracy: 65.0%\n",
      "   Correct: 130 / 200\n",
      "\n",
      "ðŸ“ˆ Per-Star Accuracy:\n",
      "   1â­:  55.0% (22 / 40)\n",
      "   2â­:  62.5% (25 / 40)\n",
      "   3â­:  67.5% (27 / 40)\n",
      "   4â­:  75.0% (30 / 40)\n",
      "   5â­:  65.0% (26 / 40)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Œ PROMPT V3\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸ”— JSON Validity Rate: 100.0%\n",
      "   Valid: 200 / 200\n",
      "\n",
      "ðŸŽ¯ Overall Accuracy: 68.5%\n",
      "   Correct: 137 / 200\n",
      "\n",
      "ðŸ“ˆ Per-Star Accuracy:\n",
      "   1â­:  67.5% (27 / 40)\n",
      "   2â­:  62.5% (25 / 40)\n",
      "   3â­:  70.0% (28 / 40)\n",
      "   4â­:  70.0% (28 / 40)\n",
      "   5â­:  72.5% (29 / 40)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics for all prompts\n",
    "print(\"ðŸ“Š EVALUATION METRICS - ALL PROMPTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store metrics for comparison table\n",
    "metrics_summary = []\n",
    "\n",
    "for prompt_version, df_results in all_results.items():\n",
    "    print(f\"\\n{'â”€' * 60}\")\n",
    "    print(f\"ðŸ“Œ PROMPT {prompt_version.upper()}\")\n",
    "    print(f\"{'â”€' * 60}\")\n",
    "    \n",
    "    # Filter to valid predictions only\n",
    "    df_valid = df_results[df_results['json_valid']].copy()\n",
    "    \n",
    "    # JSON Validity Rate\n",
    "    json_validity_rate = df_results['json_valid'].mean() * 100\n",
    "    print(f\"\\nðŸ”— JSON Validity Rate: {json_validity_rate:.1f}%\")\n",
    "    print(f\"   Valid: {df_results['json_valid'].sum()} / {len(df_results)}\")\n",
    "    \n",
    "    # Overall Accuracy (valid predictions only)\n",
    "    if len(df_valid) > 0:\n",
    "        correct_predictions = (df_valid['actual_stars'] == df_valid['predicted_stars']).sum()\n",
    "        accuracy = correct_predictions / len(df_valid) * 100\n",
    "        print(f\"\\nðŸŽ¯ Overall Accuracy: {accuracy:.1f}%\")\n",
    "        print(f\"   Correct: {correct_predictions} / {len(df_valid)}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No valid predictions to evaluate accuracy\")\n",
    "        accuracy = 0\n",
    "    \n",
    "    # Per-star accuracy\n",
    "    print(f\"\\nðŸ“ˆ Per-Star Accuracy:\")\n",
    "    per_star_acc = {}\n",
    "    for star in range(1, 6):\n",
    "        star_df = df_valid[df_valid['actual_stars'] == star]\n",
    "        if len(star_df) > 0:\n",
    "            star_correct = (star_df['actual_stars'] == star_df['predicted_stars']).sum()\n",
    "            star_accuracy = star_correct / len(star_df) * 100\n",
    "            per_star_acc[star] = star_accuracy\n",
    "            print(f\"   {star}â­: {star_accuracy:5.1f}% ({star_correct:2d} / {len(star_df):2d})\")\n",
    "        else:\n",
    "            per_star_acc[star] = 0\n",
    "            print(f\"   {star}â­: N/A (no samples)\")\n",
    "    \n",
    "    # Store for comparison\n",
    "    metrics_summary.append({\n",
    "        \"prompt\": prompt_version.upper(),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"json_validity\": json_validity_rate,\n",
    "        \"1_star_acc\": per_star_acc.get(1, 0),\n",
    "        \"2_star_acc\": per_star_acc.get(2, 0),\n",
    "        \"3_star_acc\": per_star_acc.get(3, 0),\n",
    "        \"4_star_acc\": per_star_acc.get(4, 0),\n",
    "        \"5_star_acc\": per_star_acc.get(5, 0)\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š COMPARISON TABLE\n",
      "============================================================\n",
      "prompt  accuracy  json_validity  1_star_acc  2_star_acc  3_star_acc  4_star_acc  5_star_acc\n",
      "    V1      69.5          100.0        70.0        60.0        67.5        75.0        75.0\n",
      "    V2      65.0          100.0        55.0        62.5        67.5        75.0        65.0\n",
      "    V3      68.5          100.0        67.5        62.5        70.0        70.0        72.5\n"
     ]
    }
   ],
   "source": [
    "# Create comparison DataFrame\n",
    "df_comparison = pd.DataFrame(metrics_summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ“Š COMPARISON TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_comparison.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Per-Star Accuracy Breakdown\n",
      "============================================================\n",
      "\n",
      "ðŸ“Œ Prompt V1:\n",
      "----------------------------------------\n",
      "   1â­:  70.0% (28 / 40)\n",
      "   2â­:  60.0% (24 / 40)\n",
      "   3â­:  67.5% (27 / 40)\n",
      "   4â­:  75.0% (30 / 40)\n",
      "   5â­:  75.0% (30 / 40)\n",
      "\n",
      "ðŸ“Œ Prompt V2:\n",
      "----------------------------------------\n",
      "   1â­:  55.0% (22 / 40)\n",
      "   2â­:  62.5% (25 / 40)\n",
      "   3â­:  67.5% (27 / 40)\n",
      "   4â­:  75.0% (30 / 40)\n",
      "   5â­:  65.0% (26 / 40)\n",
      "\n",
      "ðŸ“Œ Prompt V3:\n",
      "----------------------------------------\n",
      "   1â­:  67.5% (27 / 40)\n",
      "   2â­:  62.5% (25 / 40)\n",
      "   3â­:  70.0% (28 / 40)\n",
      "   4â­:  70.0% (28 / 40)\n",
      "   5â­:  72.5% (29 / 40)\n"
     ]
    }
   ],
   "source": [
    "# Per-star accuracy breakdown for all prompts\n",
    "print(\"\\nðŸ“ˆ Per-Star Accuracy Breakdown\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt_version, df_results in all_results.items():\n",
    "    df_valid = df_results[df_results['json_valid']].copy()\n",
    "    \n",
    "    print(f\"\\nðŸ“Œ Prompt {prompt_version.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for star in range(1, 6):\n",
    "        star_df = df_valid[df_valid['actual_stars'] == star]\n",
    "        if len(star_df) > 0:\n",
    "            star_correct = (star_df['actual_stars'] == star_df['predicted_stars']).sum()\n",
    "            star_accuracy = star_correct / len(star_df) * 100\n",
    "            print(f\"   {star}â­: {star_accuracy:5.1f}% ({star_correct:2d} / {len(star_df):2d})\")\n",
    "        else:\n",
    "            print(f\"   {star}â­: N/A (no samples)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Prediction Distribution - All Prompts\n",
      "============================================================\n",
      "\n",
      "ðŸ“Œ Prompt V1:\n",
      "----------------------------------------\n",
      "Star   Actual     Predicted \n",
      "--------------------------\n",
      "1â­    40         36        \n",
      "2â­    40         42        \n",
      "3â­    40         40        \n",
      "4â­    40         45        \n",
      "5â­    40         37        \n",
      "\n",
      "ðŸ“Œ Prompt V2:\n",
      "----------------------------------------\n",
      "Star   Actual     Predicted \n",
      "--------------------------\n",
      "1â­    40         30        \n",
      "2â­    40         50        \n",
      "3â­    40         44        \n",
      "4â­    40         47        \n",
      "5â­    40         29        \n",
      "\n",
      "ðŸ“Œ Prompt V3:\n",
      "----------------------------------------\n",
      "Star   Actual     Predicted \n",
      "--------------------------\n",
      "1â­    40         35        \n",
      "2â­    40         44        \n",
      "3â­    40         44        \n",
      "4â­    40         42        \n",
      "5â­    40         35        \n"
     ]
    }
   ],
   "source": [
    "# Prediction distribution for all prompts\n",
    "print(\"\\nðŸ“Š Prediction Distribution - All Prompts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt_version, df_results in all_results.items():\n",
    "    df_valid = df_results[df_results['json_valid']].copy()\n",
    "    \n",
    "    print(f\"\\nðŸ“Œ Prompt {prompt_version.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if len(df_valid) > 0:\n",
    "        pred_counts = df_valid['predicted_stars'].value_counts().sort_index()\n",
    "        actual_counts = df_valid['actual_stars'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\"{'Star':<6} {'Actual':<10} {'Predicted':<10}\")\n",
    "        print(\"-\" * 26)\n",
    "        for star in range(1, 6):\n",
    "            actual = actual_counts.get(star, 0)\n",
    "            predicted = pred_counts.get(star, 0)\n",
    "            print(f\"{star}â­    {actual:<10} {predicted:<10}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Confusion Matrices (Actual vs Predicted)\n",
      "============================================================\n",
      "\n",
      "ðŸ“Œ Prompt V1:\n",
      "--------------------------------------------------\n",
      "Predicted   1   2   3   4   5\n",
      "Actual                       \n",
      "1          28  12   0   0   0\n",
      "2           7  24   9   0   0\n",
      "3           0   6  27   7   0\n",
      "4           0   0   3  30   7\n",
      "5           1   0   1   8  30\n",
      "\n",
      "ðŸ“Œ Prompt V2:\n",
      "--------------------------------------------------\n",
      "Predicted   1   2   3   4   5\n",
      "Actual                       \n",
      "1          22  18   0   0   0\n",
      "2           7  25   8   0   0\n",
      "3           0   7  27   6   0\n",
      "4           0   0   7  30   3\n",
      "5           1   0   2  11  26\n",
      "\n",
      "ðŸ“Œ Prompt V3:\n",
      "--------------------------------------------------\n",
      "Predicted   1   2   3   4   5\n",
      "Actual                       \n",
      "1          27  13   0   0   0\n",
      "2           7  25   8   0   0\n",
      "3           0   6  28   6   0\n",
      "4           0   0   6  28   6\n",
      "5           1   0   2   8  29\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix for all prompts\n",
    "print(\"\\nðŸ“‹ Confusion Matrices (Actual vs Predicted)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt_version, df_results in all_results.items():\n",
    "    df_valid = df_results[df_results['json_valid']].copy()\n",
    "    \n",
    "    print(f\"\\nðŸ“Œ Prompt {prompt_version.upper()}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if len(df_valid) > 0:\n",
    "        confusion = pd.crosstab(\n",
    "            df_valid['actual_stars'], \n",
    "            df_valid['predicted_stars'],\n",
    "            rownames=['Actual'],\n",
    "            colnames=['Predicted']\n",
    "        )\n",
    "        print(confusion)\n",
    "    else:\n",
    "        print(\"No valid predictions for confusion matrix\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
